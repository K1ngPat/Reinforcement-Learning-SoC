{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 64\n",
    "N = 8 # board size\n",
    "\n",
    "num_input_planes =  (2*6 + 1) + (1 + 4 + 1)\n",
    "INPUT_DIM = (BATCH_SIZE, num_input_planes, N, N) # (64, 19, 8, 8)\n",
    "queen_planes = 56\n",
    "knight_planes = 8\n",
    "underpromotion_planes = 9\n",
    "num_output_planes = queen_planes + knight_planes + underpromotion_planes\n",
    "OUTPUT_DIM = (BATCH_SIZE, N*N*num_output_planes, 1) # (64, 73x8x8, 1)\n",
    "\n",
    "LEARNING_RATE = 0.2\n",
    "POLICY_WEIGHT = 0.5 # weight of policy loss\n",
    "VALUE_WEIGHT = 0.5 # weight of value loss\n",
    "CONVOLUTION_FILTERS = 256\n",
    "NUM_HIDDEN_LAYERS = 19 # number of residual blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_hidden_layers, convolution_filters):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.convolution_filters = convolution_filters\n",
    "\n",
    "        # convolutional layer\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.input_dim[1], out_channels=self.convolution_filters, kernel_size=3, stride=(1, 1), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(num_features=self.convolution_filters),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # residual layer\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.convolution_filters, out_channels=convolution_filters, kernel_size=3, stride=(1, 1), padding=1),\n",
    "            nn.BatchNorm2d(num_features=self.convolution_filters),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=self.convolution_filters, out_channels=convolution_filters, kernel_size=3, stride=(1, 1), padding=1),\n",
    "            nn.BatchNorm2d(self.convolution_filters)\n",
    "        )\n",
    "        self.residual_layers = nn.ModuleList()\n",
    "        for _ in range(num_hidden_layers):\n",
    "            self.residual_layers.append(self.residual)\n",
    "\n",
    "        # policy head\n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.convolution_filters, out_channels=2, kernel_size=(1, 1), padding=0, stride=(1, 1)),\n",
    "            nn.BatchNorm2d(num_features=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=2*self.input_dim[2]*self.input_dim[3], out_features=self.output_dim[1]),\n",
    "            nn.Sigmoid() # probability = (0, 1)\n",
    "        )\n",
    "\n",
    "        # value head\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.convolution_filters, out_channels=1, kernel_size=(1, 1), padding=0, stride=(1, 1)),\n",
    "            nn.BatchNorm2d(num_features=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=1*self.input_dim[2]*self.input_dim[3], out_features=self.convolution_filters),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=self.convolution_filters, out_features=1),\n",
    "            nn.Tanh() # value = (-1, 1)\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # convolutional layer\n",
    "        x = self.conv(x)\n",
    "\n",
    "        # residual layers\n",
    "        for residual_layer in self.residual_layers:\n",
    "            x_res = residual_layer(x)\n",
    "            x += x_res\n",
    "            x = torch.relu(x)\n",
    "\n",
    "        # policy output\n",
    "        policy_output = self.policy_head(x)\n",
    "\n",
    "        # value output\n",
    "        value_output = self.value_head(x)\n",
    "\n",
    "        return policy_output, value_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 256, 8, 8]          44,032\n",
      "       BatchNorm2d-2            [-1, 256, 8, 8]             512\n",
      "              ReLU-3            [-1, 256, 8, 8]               0\n",
      "            Conv2d-4            [-1, 256, 8, 8]         590,080\n",
      "            Conv2d-5            [-1, 256, 8, 8]         590,080\n",
      "       BatchNorm2d-6            [-1, 256, 8, 8]             512\n",
      "       BatchNorm2d-7            [-1, 256, 8, 8]             512\n",
      "              ReLU-8            [-1, 256, 8, 8]               0\n",
      "              ReLU-9            [-1, 256, 8, 8]               0\n",
      "           Conv2d-10            [-1, 256, 8, 8]         590,080\n",
      "           Conv2d-11            [-1, 256, 8, 8]         590,080\n",
      "      BatchNorm2d-12            [-1, 256, 8, 8]             512\n",
      "      BatchNorm2d-13            [-1, 256, 8, 8]             512\n",
      "           Conv2d-14            [-1, 256, 8, 8]         590,080\n",
      "           Conv2d-15            [-1, 256, 8, 8]         590,080\n",
      "      BatchNorm2d-16            [-1, 256, 8, 8]             512\n",
      "      BatchNorm2d-17            [-1, 256, 8, 8]             512\n",
      "             ReLU-18            [-1, 256, 8, 8]               0\n",
      "             ReLU-19            [-1, 256, 8, 8]               0\n",
      "           Conv2d-20            [-1, 256, 8, 8]         590,080\n",
      "           Conv2d-21            [-1, 256, 8, 8]         590,080\n",
      "      BatchNorm2d-22            [-1, 256, 8, 8]             512\n",
      "      BatchNorm2d-23            [-1, 256, 8, 8]             512\n",
      "           Conv2d-24            [-1, 256, 8, 8]         590,080\n",
      "           Conv2d-25            [-1, 256, 8, 8]         590,080\n",
      "      BatchNorm2d-26            [-1, 256, 8, 8]             512\n",
      "      BatchNorm2d-27            [-1, 256, 8, 8]             512\n",
      "             ReLU-28            [-1, 256, 8, 8]               0\n",
      "             ReLU-29            [-1, 256, 8, 8]               0\n",
      "           Conv2d-30            [-1, 256, 8, 8]         590,080\n",
      "           Conv2d-31            [-1, 256, 8, 8]         590,080\n",
      "      BatchNorm2d-32            [-1, 256, 8, 8]             512\n",
      "      BatchNorm2d-33            [-1, 256, 8, 8]             512\n",
      "           Conv2d-34            [-1, 256, 8, 8]         590,080\n",
      "           Conv2d-35            [-1, 256, 8, 8]         590,080\n",
      "      BatchNorm2d-36            [-1, 256, 8, 8]             512\n",
      "      BatchNorm2d-37            [-1, 256, 8, 8]             512\n",
      "             ReLU-38            [-1, 256, 8, 8]               0\n",
      "             ReLU-39            [-1, 256, 8, 8]               0\n",
      "           Conv2d-40            [-1, 256, 8, 8]         590,080\n",
      "           Conv2d-41            [-1, 256, 8, 8]         590,080\n",
      "      BatchNorm2d-42            [-1, 256, 8, 8]             512\n",
      "      BatchNorm2d-43            [-1, 256, 8, 8]             512\n",
      "           Conv2d-44            [-1, 256, 8, 8]         590,080\n",
      "           Conv2d-45            [-1, 256, 8, 8]         590,080\n",
      "      BatchNorm2d-46            [-1, 256, 8, 8]             512\n",
      "      BatchNorm2d-47            [-1, 256, 8, 8]             512\n",
      "             ReLU-48            [-1, 256, 8, 8]               0\n",
      "             ReLU-49            [-1, 256, 8, 8]               0\n",
      "           Conv2d-50            [-1, 256, 8, 8]         590,080\n",
      "           Conv2d-51            [-1, 256, 8, 8]         590,080\n",
      "      BatchNorm2d-52            [-1, 256, 8, 8]             512\n",
      "      BatchNorm2d-53            [-1, 256, 8, 8]             512\n",
      "           Conv2d-54            [-1, 256, 8, 8]         590,080\n",
      "           Conv2d-55            [-1, 256, 8, 8]         590,080\n",
      "      BatchNorm2d-56            [-1, 256, 8, 8]             512\n",
      "      BatchNorm2d-57            [-1, 256, 8, 8]             512\n",
      "             ReLU-58            [-1, 256, 8, 8]               0\n",
      "             ReLU-59            [-1, 256, 8, 8]               0\n",
      "           Conv2d-60            [-1, 256, 8, 8]         590,080\n",
      "           Conv2d-61            [-1, 256, 8, 8]         590,080\n",
      "      BatchNorm2d-62            [-1, 256, 8, 8]             512\n",
      "      BatchNorm2d-63            [-1, 256, 8, 8]             512\n",
      "           Conv2d-64            [-1, 256, 8, 8]         590,080\n",
      "           Conv2d-65            [-1, 256, 8, 8]         590,080\n",
      "      BatchNorm2d-66            [-1, 256, 8, 8]             512\n",
      "      BatchNorm2d-67            [-1, 256, 8, 8]             512\n",
      "             ReLU-68            [-1, 256, 8, 8]               0\n",
      "             ReLU-69            [-1, 256, 8, 8]               0\n",
      "           Conv2d-70            [-1, 256, 8, 8]         590,080\n",
      "           Conv2d-71            [-1, 256, 8, 8]         590,080\n",
      "      BatchNorm2d-72            [-1, 256, 8, 8]             512\n",
      "      BatchNorm2d-73            [-1, 256, 8, 8]             512\n",
      "           Conv2d-74            [-1, 256, 8, 8]         590,080\n",
      "           Conv2d-75            [-1, 256, 8, 8]         590,080\n",
      "      BatchNorm2d-76            [-1, 256, 8, 8]             512\n",
      "      BatchNorm2d-77            [-1, 256, 8, 8]             512\n",
      "             ReLU-78            [-1, 256, 8, 8]               0\n",
      "             ReLU-79            [-1, 256, 8, 8]               0\n",
      "           Conv2d-80            [-1, 256, 8, 8]         590,080\n",
      "           Conv2d-81            [-1, 256, 8, 8]         590,080\n",
      "      BatchNorm2d-82            [-1, 256, 8, 8]             512\n",
      "      BatchNorm2d-83            [-1, 256, 8, 8]             512\n",
      "           Conv2d-84            [-1, 256, 8, 8]         590,080\n",
      "           Conv2d-85            [-1, 256, 8, 8]         590,080\n",
      "      BatchNorm2d-86            [-1, 256, 8, 8]             512\n",
      "      BatchNorm2d-87            [-1, 256, 8, 8]             512\n",
      "             ReLU-88            [-1, 256, 8, 8]               0\n",
      "             ReLU-89            [-1, 256, 8, 8]               0\n",
      "           Conv2d-90            [-1, 256, 8, 8]         590,080\n",
      "           Conv2d-91            [-1, 256, 8, 8]         590,080\n",
      "      BatchNorm2d-92            [-1, 256, 8, 8]             512\n",
      "      BatchNorm2d-93            [-1, 256, 8, 8]             512\n",
      "           Conv2d-94            [-1, 256, 8, 8]         590,080\n",
      "           Conv2d-95            [-1, 256, 8, 8]         590,080\n",
      "      BatchNorm2d-96            [-1, 256, 8, 8]             512\n",
      "      BatchNorm2d-97            [-1, 256, 8, 8]             512\n",
      "             ReLU-98            [-1, 256, 8, 8]               0\n",
      "             ReLU-99            [-1, 256, 8, 8]               0\n",
      "          Conv2d-100            [-1, 256, 8, 8]         590,080\n",
      "          Conv2d-101            [-1, 256, 8, 8]         590,080\n",
      "     BatchNorm2d-102            [-1, 256, 8, 8]             512\n",
      "     BatchNorm2d-103            [-1, 256, 8, 8]             512\n",
      "          Conv2d-104            [-1, 256, 8, 8]         590,080\n",
      "          Conv2d-105            [-1, 256, 8, 8]         590,080\n",
      "     BatchNorm2d-106            [-1, 256, 8, 8]             512\n",
      "     BatchNorm2d-107            [-1, 256, 8, 8]             512\n",
      "            ReLU-108            [-1, 256, 8, 8]               0\n",
      "            ReLU-109            [-1, 256, 8, 8]               0\n",
      "          Conv2d-110            [-1, 256, 8, 8]         590,080\n",
      "          Conv2d-111            [-1, 256, 8, 8]         590,080\n",
      "     BatchNorm2d-112            [-1, 256, 8, 8]             512\n",
      "     BatchNorm2d-113            [-1, 256, 8, 8]             512\n",
      "          Conv2d-114            [-1, 256, 8, 8]         590,080\n",
      "          Conv2d-115            [-1, 256, 8, 8]         590,080\n",
      "     BatchNorm2d-116            [-1, 256, 8, 8]             512\n",
      "     BatchNorm2d-117            [-1, 256, 8, 8]             512\n",
      "            ReLU-118            [-1, 256, 8, 8]               0\n",
      "            ReLU-119            [-1, 256, 8, 8]               0\n",
      "          Conv2d-120            [-1, 256, 8, 8]         590,080\n",
      "          Conv2d-121            [-1, 256, 8, 8]         590,080\n",
      "     BatchNorm2d-122            [-1, 256, 8, 8]             512\n",
      "     BatchNorm2d-123            [-1, 256, 8, 8]             512\n",
      "          Conv2d-124            [-1, 256, 8, 8]         590,080\n",
      "          Conv2d-125            [-1, 256, 8, 8]         590,080\n",
      "     BatchNorm2d-126            [-1, 256, 8, 8]             512\n",
      "     BatchNorm2d-127            [-1, 256, 8, 8]             512\n",
      "            ReLU-128            [-1, 256, 8, 8]               0\n",
      "            ReLU-129            [-1, 256, 8, 8]               0\n",
      "          Conv2d-130            [-1, 256, 8, 8]         590,080\n",
      "          Conv2d-131            [-1, 256, 8, 8]         590,080\n",
      "     BatchNorm2d-132            [-1, 256, 8, 8]             512\n",
      "     BatchNorm2d-133            [-1, 256, 8, 8]             512\n",
      "          Conv2d-134            [-1, 256, 8, 8]         590,080\n",
      "          Conv2d-135            [-1, 256, 8, 8]         590,080\n",
      "     BatchNorm2d-136            [-1, 256, 8, 8]             512\n",
      "     BatchNorm2d-137            [-1, 256, 8, 8]             512\n",
      "            ReLU-138            [-1, 256, 8, 8]               0\n",
      "            ReLU-139            [-1, 256, 8, 8]               0\n",
      "          Conv2d-140            [-1, 256, 8, 8]         590,080\n",
      "          Conv2d-141            [-1, 256, 8, 8]         590,080\n",
      "     BatchNorm2d-142            [-1, 256, 8, 8]             512\n",
      "     BatchNorm2d-143            [-1, 256, 8, 8]             512\n",
      "          Conv2d-144            [-1, 256, 8, 8]         590,080\n",
      "          Conv2d-145            [-1, 256, 8, 8]         590,080\n",
      "     BatchNorm2d-146            [-1, 256, 8, 8]             512\n",
      "     BatchNorm2d-147            [-1, 256, 8, 8]             512\n",
      "            ReLU-148            [-1, 256, 8, 8]               0\n",
      "            ReLU-149            [-1, 256, 8, 8]               0\n",
      "          Conv2d-150            [-1, 256, 8, 8]         590,080\n",
      "          Conv2d-151            [-1, 256, 8, 8]         590,080\n",
      "     BatchNorm2d-152            [-1, 256, 8, 8]             512\n",
      "     BatchNorm2d-153            [-1, 256, 8, 8]             512\n",
      "          Conv2d-154            [-1, 256, 8, 8]         590,080\n",
      "          Conv2d-155            [-1, 256, 8, 8]         590,080\n",
      "     BatchNorm2d-156            [-1, 256, 8, 8]             512\n",
      "     BatchNorm2d-157            [-1, 256, 8, 8]             512\n",
      "            ReLU-158            [-1, 256, 8, 8]               0\n",
      "            ReLU-159            [-1, 256, 8, 8]               0\n",
      "          Conv2d-160            [-1, 256, 8, 8]         590,080\n",
      "          Conv2d-161            [-1, 256, 8, 8]         590,080\n",
      "     BatchNorm2d-162            [-1, 256, 8, 8]             512\n",
      "     BatchNorm2d-163            [-1, 256, 8, 8]             512\n",
      "          Conv2d-164            [-1, 256, 8, 8]         590,080\n",
      "          Conv2d-165            [-1, 256, 8, 8]         590,080\n",
      "     BatchNorm2d-166            [-1, 256, 8, 8]             512\n",
      "     BatchNorm2d-167            [-1, 256, 8, 8]             512\n",
      "            ReLU-168            [-1, 256, 8, 8]               0\n",
      "            ReLU-169            [-1, 256, 8, 8]               0\n",
      "          Conv2d-170            [-1, 256, 8, 8]         590,080\n",
      "          Conv2d-171            [-1, 256, 8, 8]         590,080\n",
      "     BatchNorm2d-172            [-1, 256, 8, 8]             512\n",
      "     BatchNorm2d-173            [-1, 256, 8, 8]             512\n",
      "          Conv2d-174            [-1, 256, 8, 8]         590,080\n",
      "          Conv2d-175            [-1, 256, 8, 8]         590,080\n",
      "     BatchNorm2d-176            [-1, 256, 8, 8]             512\n",
      "     BatchNorm2d-177            [-1, 256, 8, 8]             512\n",
      "            ReLU-178            [-1, 256, 8, 8]               0\n",
      "            ReLU-179            [-1, 256, 8, 8]               0\n",
      "          Conv2d-180            [-1, 256, 8, 8]         590,080\n",
      "          Conv2d-181            [-1, 256, 8, 8]         590,080\n",
      "     BatchNorm2d-182            [-1, 256, 8, 8]             512\n",
      "     BatchNorm2d-183            [-1, 256, 8, 8]             512\n",
      "          Conv2d-184            [-1, 256, 8, 8]         590,080\n",
      "          Conv2d-185            [-1, 256, 8, 8]         590,080\n",
      "     BatchNorm2d-186            [-1, 256, 8, 8]             512\n",
      "     BatchNorm2d-187            [-1, 256, 8, 8]             512\n",
      "            ReLU-188            [-1, 256, 8, 8]               0\n",
      "            ReLU-189            [-1, 256, 8, 8]               0\n",
      "          Conv2d-190            [-1, 256, 8, 8]         590,080\n",
      "          Conv2d-191            [-1, 256, 8, 8]         590,080\n",
      "     BatchNorm2d-192            [-1, 256, 8, 8]             512\n",
      "     BatchNorm2d-193            [-1, 256, 8, 8]             512\n",
      "          Conv2d-194              [-1, 2, 8, 8]             514\n",
      "     BatchNorm2d-195              [-1, 2, 8, 8]               4\n",
      "            ReLU-196              [-1, 2, 8, 8]               0\n",
      "         Flatten-197                  [-1, 128]               0\n",
      "          Linear-198                 [-1, 4672]         602,688\n",
      "         Sigmoid-199                 [-1, 4672]               0\n",
      "          Conv2d-200              [-1, 1, 8, 8]             257\n",
      "     BatchNorm2d-201              [-1, 1, 8, 8]               2\n",
      "            ReLU-202              [-1, 1, 8, 8]               0\n",
      "         Flatten-203                   [-1, 64]               0\n",
      "          Linear-204                  [-1, 256]          16,640\n",
      "            ReLU-205                  [-1, 256]               0\n",
      "          Linear-206                    [-1, 1]             257\n",
      "            Tanh-207                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 45,549,898\n",
      "Trainable params: 45,549,898\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 24.21\n",
      "Params size (MB): 173.76\n",
      "Estimated Total Size (MB): 197.97\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "model = ConvNet(INPUT_DIM, OUTPUT_DIM, NUM_HIDDEN_LAYERS, CONVOLUTION_FILTERS)\n",
    "input_size = INPUT_DIM[1:]\n",
    "summary(model, input_size=tuple(input_size), device='cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = INPUT_DIM\n",
    "output_dim = OUTPUT_DIM\n",
    "num_hidden_layers = NUM_HIDDEN_LAYERS\n",
    "convolution_filters = CONVOLUTION_FILTERS\n",
    "\n",
    "model = ConvNet(input_dim, output_dim, num_hidden_layers, convolution_filters)\n",
    "\n",
    "policy_criterion = nn.CrossEntropyLoss()\n",
    "value_criterion = nn.MSELoss()\n",
    "\n",
    "learning_rate = LEARNING_RATE\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def train_model():\n",
    "    epochs = EPOCHS\n",
    "    policy_weight = POLICY_WEIGHT\n",
    "    value_weight = VALUE_WEIGHT\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # to be written on the basis of input data\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
